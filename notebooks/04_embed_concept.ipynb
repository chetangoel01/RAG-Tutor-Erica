{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9cf034e",
   "metadata": {},
   "source": [
    "# M4: Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2793270b",
   "metadata": {},
   "source": [
    "## Concepts Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b681869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ChromaDB Statistics\n",
      "============================================================\n",
      "‚ö†Ô∏è  Collection 'concepts' does not exist yet.\n",
      "\n",
      "üìù Next steps:\n",
      "   1. Make sure MongoDB has concepts (check the knowledge graph notebook)\n",
      "   2. Run the embedding cell below to create the collection and embed concepts\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Query ChromaDB - Check how many concepts are stored\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/app')\n",
    "\n",
    "import chromadb\n",
    "\n",
    "try:\n",
    "    client = chromadb.HttpClient(host=\"chromadb\", port=8000)\n",
    "    collection_name = \"concepts\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"ChromaDB Statistics\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check if collection exists\n",
    "    try:\n",
    "        collection = client.get_collection(collection_name)\n",
    "        count = collection.count()\n",
    "        metadata = collection.metadata\n",
    "        \n",
    "        print(f\"‚úÖ Collection: {collection_name}\")\n",
    "        print(f\"‚úÖ Total Concepts: {count}\")\n",
    "        print(f\"Metadata: {metadata}\")\n",
    "        \n",
    "        if count == 0:\n",
    "            print(\"\\n‚ö†Ô∏è  ChromaDB collection exists but is empty!\")\n",
    "            print(\"   Run the embedding cell below to populate it.\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ ChromaDB has {count} concepts embedded and ready for semantic search!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        if \"does not exist\" in str(e) or \"not found\" in str(e).lower():\n",
    "            print(f\"‚ö†Ô∏è  Collection '{collection_name}' does not exist yet.\")\n",
    "            print(\"\\nüìù Next steps:\")\n",
    "            print(\"   1. Make sure MongoDB has concepts (check the knowledge graph notebook)\")\n",
    "            print(\"   2. Run the embedding cell below to create the collection and embed concepts\")\n",
    "        else:\n",
    "            print(f\"‚ùå Error accessing collection: {e}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ChromaDB Statistics\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"‚ùå Cannot connect to ChromaDB: {e}\")\n",
    "    print(\"\\nüîß Troubleshooting:\")\n",
    "    print(\"   1. Make sure ChromaDB container is running:\")\n",
    "    print(\"      docker-compose up -d chromadb\")\n",
    "    print(\"   2. Check container status:\")\n",
    "    print(\"      docker-compose ps chromadb\")\n",
    "    print(\"   3. Check logs:\")\n",
    "    print(\"      docker-compose logs chromadb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b494ec6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2...\n",
      "Model loaded. Embedding dimension: 384\n",
      "Found 3120 concepts in MongoDB\n",
      "  Embedded 100/3120 concepts\n",
      "  Embedded 200/3120 concepts\n",
      "  Embedded 300/3120 concepts\n",
      "  Embedded 400/3120 concepts\n",
      "  Embedded 500/3120 concepts\n",
      "  Embedded 600/3120 concepts\n",
      "  Embedded 700/3120 concepts\n",
      "  Embedded 800/3120 concepts\n",
      "  Embedded 900/3120 concepts\n",
      "  Embedded 1000/3120 concepts\n",
      "  Embedded 1100/3120 concepts\n",
      "  Embedded 1200/3120 concepts\n",
      "  Embedded 1300/3120 concepts\n",
      "  Embedded 1400/3120 concepts\n",
      "  Embedded 1500/3120 concepts\n",
      "  Embedded 1600/3120 concepts\n",
      "  Embedded 1700/3120 concepts\n",
      "  Embedded 1800/3120 concepts\n",
      "  Embedded 1900/3120 concepts\n",
      "  Embedded 2000/3120 concepts\n",
      "  Embedded 2100/3120 concepts\n",
      "  Embedded 2200/3120 concepts\n",
      "  Embedded 2300/3120 concepts\n",
      "  Embedded 2400/3120 concepts\n",
      "  Embedded 2500/3120 concepts\n",
      "  Embedded 2600/3120 concepts\n",
      "  Embedded 2700/3120 concepts\n",
      "  Embedded 2800/3120 concepts\n",
      "  Embedded 2900/3120 concepts\n",
      "  Embedded 3000/3120 concepts\n",
      "  Embedded 3100/3120 concepts\n",
      "  Embedded 3120/3120 concepts\n",
      "\n",
      "Done! 3120 concepts embedded into ChromaDB collection 'concepts'\n",
      "\n",
      "==================================================\n",
      "ChromaDB Stats:\n",
      "{'collection': 'concepts', 'count': 3120, 'metadata': {'description': 'Concept embeddings for semantic search'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2...\n",
      "Model loaded. Embedding dimension: 384\n",
      "  Stopping...\n",
      "  Stopping...\n",
      "  Stopping...\n",
      "  Stopping...\n",
      "  Stopping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in atexit callback: <function dump_compile_times at 0xffff0c9a7e20>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 845, in dump_compile_times\n",
      "    log.info(compile_times(repr=\"str\", aggregate=True))\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 831, in compile_times\n",
      "    out += tabulate(rows, headers=(\"Function\", \"Runtimes (s)\"))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 237, in tabulate\n",
      "    import tabulate\n",
      "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1138, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 1070, in _find_spec\n",
      "  File \"/usr/local/lib/python3.11/site-packages/streamlit/web/bootstrap.py\", line 42, in signal_handler\n",
      "    server.stop()\n",
      "  File \"/usr/local/lib/python3.11/site-packages/streamlit/web/server/server.py\", line 519, in stop\n",
      "    self._runtime.stop()\n",
      "  File \"/usr/local/lib/python3.11/site-packages/streamlit/runtime/runtime.py\", line 343, in stop\n",
      "    async_objs.eventloop.call_soon_threadsafe(stop_on_eventloop)\n",
      "  File \"/usr/local/lib/python3.11/asyncio/base_events.py\", line 807, in call_soon_threadsafe\n",
      "    self._check_closed()\n",
      "  File \"/usr/local/lib/python3.11/asyncio/base_events.py\", line 520, in _check_closed\n",
      "    raise RuntimeError('Event loop is closed')\n",
      "RuntimeError: Event loop is closed\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/weakref.py\", line 666, in _exitfunc\n",
      "    f()\n",
      "  File \"/usr/local/lib/python3.11/weakref.py\", line 590, in __call__\n",
      "    return info.func(*info.args, **(info.kwargs or {}))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/library.py\", line 462, in _del_library\n",
      "    m.reset()\n",
      "  File \"/usr/local/lib/python3.11/site-packages/streamlit/web/bootstrap.py\", line 42, in signal_handler\n",
      "    server.stop()\n",
      "  File \"/usr/local/lib/python3.11/site-packages/streamlit/web/server/server.py\", line 519, in stop\n",
      "    self._runtime.stop()\n",
      "  File \"/usr/local/lib/python3.11/site-packages/streamlit/runtime/runtime.py\", line 343, in stop\n",
      "    async_objs.eventloop.call_soon_threadsafe(stop_on_eventloop)\n",
      "  File \"/usr/local/lib/python3.11/asyncio/base_events.py\", line 807, in call_soon_threadsafe\n",
      "    self._check_closed()\n",
      "  File \"/usr/local/lib/python3.11/asyncio/base_events.py\", line 520, in _check_closed\n",
      "    raise RuntimeError('Event loop is closed')\n",
      "RuntimeError: Event loop is closed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Stopping...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# M4: Embed Concepts into ChromaDB\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/app')\n",
    "\n",
    "from src.retrieval.concept_embeddings import ConceptEmbedder\n",
    "\n",
    "# Initialize embedder\n",
    "embedder = ConceptEmbedder(\n",
    "    mongo_uri=\"mongodb://erica:erica_password_123@mongodb:27017/\",\n",
    "    chroma_host=\"chromadb\",\n",
    "    chroma_port=8000,\n",
    ")\n",
    "\n",
    "# Embed all concepts (takes ~1-2 minutes for 3120 concepts)\n",
    "embedder.embed_all_concepts(clear_existing=True)\n",
    "\n",
    "# Check stats\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ChromaDB Stats:\")\n",
    "print(embedder.get_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d8a5b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Query: What is backpropagation?\n",
      "============================================================\n",
      "\n",
      "1. Back-Propagation (score: 0.6454, difficulty: intermediate)\n",
      "   An algorithm used to compute the gradient of the loss function with respect to the weights in a neural network, enabling the optimization of the netwo...\n",
      "\n",
      "2. Backward Propagation (score: 0.6286, difficulty: intermediate)\n",
      "   A method used in training neural networks to calculate the gradient of the loss function with respect to the weights of the network....\n",
      "\n",
      "3. Backward Pass (score: 0.6013, difficulty: intermediate)\n",
      "   The process of computing gradients of the loss function with respect to the parameters of a neural network, performed during backpropagation....\n",
      "\n",
      "4. Stochastic Backpropagation (score: 0.6004, difficulty: intermediate)\n",
      "   An optimization method used in neural network training that is faster for large and redundant problems, such as classification tasks....\n",
      "\n",
      "5. Guided Backpropagation (score: 0.5644, difficulty: intermediate)\n",
      "   A method for visualizing the important regions of an input image by backpropagating gradients through the network with modifications to preserve posit...\n",
      "\n",
      "============================================================\n",
      "Query: How do neural networks learn?\n",
      "============================================================\n",
      "\n",
      "1. Neural Networks (score: 0.5367, difficulty: beginner)\n",
      "   Neural networks are biologically inspired models of computation consisting of artificial neurons (nodes or units) and directed edges between them, rep...\n",
      "\n",
      "2. Neural Network Models (score: 0.5351, difficulty: intermediate)\n",
      "   Machine learning models inspired by the human brain, used for learning complex patterns in data....\n",
      "\n",
      "3. Neural Network Training (score: 0.5333, difficulty: beginner)\n",
      "   The process of adjusting the parameters of a neural network to minimize a loss function using data....\n",
      "\n",
      "4. Gradient-Following Heuristics (score: 0.5328, difficulty: intermediate)\n",
      "   Optimization techniques used in training neural networks to navigate the error surface and escape local optima or saddle points....\n",
      "\n",
      "5. Training (score: 0.5268, difficulty: intermediate)\n",
      "   The process of teaching a machine learning model to make accurate predictions by adjusting its parameters based on data....\n",
      "\n",
      "============================================================\n",
      "Query: Explain gradient descent\n",
      "============================================================\n",
      "\n",
      "1. Gradient Descent (score: 0.6813, difficulty: beginner)\n",
      "   An optimization algorithm used to minimize a function by iteratively moving towards the minimum value of the function, typically by taking steps propo...\n",
      "\n",
      "2. Steepest Gradient Descent (score: 0.6773, difficulty: intermediate)\n",
      "   An optimization algorithm that iteratively moves in the direction of the negative gradient to minimize a function....\n",
      "\n",
      "3. Gradient Calculation (score: 0.6206, difficulty: intermediate)\n",
      "   The process of computing the derivative of the loss function with respect to the model parameters during training....\n",
      "\n",
      "4. Gradient Computation (score: 0.6023, difficulty: intermediate)\n",
      "   The process of calculating the derivative of a loss function with respect to model parameters....\n",
      "\n",
      "5. Gradient-based Training (score: 0.5778, difficulty: intermediate)\n",
      "   A method of training neural networks by computing gradients of the loss function with respect to model parameters and updating them to minimize the lo...\n",
      "\n",
      "============================================================\n",
      "Query: What is the difference between CNN and RNN?\n",
      "============================================================\n",
      "\n",
      "1. RNN (Recurrent Neural Network) (score: 0.5656, difficulty: intermediate)\n",
      "   A type of neural network designed to process sequential data by maintaining hidden states that capture information about previous inputs in the sequen...\n",
      "\n",
      "2. RNNs (score: 0.5535, difficulty: intermediate)\n",
      "   Recurrent Neural Network is a class of neural networks that are well-suited to sequence modeling, where connections between nodes form a directed grap...\n",
      "\n",
      "3. Simple RNN (score: 0.5522, difficulty: intermediate)\n",
      "   A basic recurrent neural network architecture with a single layer of neurons that processes input sequentially and maintains a hidden state across tim...\n",
      "\n",
      "4. Deep RNNs (score: 0.5516, difficulty: intermediate)\n",
      "   Deep RNNs are recurrent neural networks with multiple RNN layers stacked on top of each other to learn complex sequential patterns....\n",
      "\n",
      "5. Stacked RNN (score: 0.5485, difficulty: intermediate)\n",
      "   An architecture where multiple RNN layers are stacked on top of each other to create a deeper model....\n",
      "\n",
      "============================================================\n",
      "Query: How does attention mechanism work in transformers?\n",
      "============================================================\n",
      "\n",
      "1. Transformer Models (score: 0.552, difficulty: intermediate)\n",
      "   A type of neural network architecture that relies on self-attention mechanisms and positional encoding to process sequential data....\n",
      "\n",
      "2. Transformer Self-Attention Networks (score: 0.5418, difficulty: intermediate)\n",
      "   A neural network architecture that uses self-attention mechanisms to process input sequences, introduced by Vaswani et al. in 2017....\n",
      "\n",
      "3. Transformers (score: 0.5341, difficulty: intermediate)\n",
      "   A neural network architecture that uses self-attention mechanisms to process sequences, enabling parallel computation and better modeling of long-rang...\n",
      "\n",
      "4. Attention (score: 0.5289, difficulty: intermediate)\n",
      "   A mechanism in neural machine translation that aligns words in the source sentence with words in the target sentence using attention scores....\n",
      "\n",
      "5. Transformer architecture (score: 0.5249, difficulty: intermediate)\n",
      "   A model architecture that uses self-attention mechanisms to process sequences, offering an alternative to RNNs and CNNs for tasks like image captionin...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Test Semantic Search\n",
    "# =============================================================================\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What is backpropagation?\",\n",
    "    \"How do neural networks learn?\",\n",
    "    \"Explain gradient descent\",\n",
    "    \"What is the difference between CNN and RNN?\",\n",
    "    \"How does attention mechanism work in transformers?\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = embedder.search(query, top_k=5)\n",
    "    \n",
    "    for i, r in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. {r['title']} (score: {r['score']}, difficulty: {r['difficulty']})\")\n",
    "        print(f\"   {r['definition'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986193fe",
   "metadata": {},
   "source": [
    "## Graph Retreival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1349f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed concepts: ['Gradient Descent']\n",
      "\n",
      "Retrieved 15 concepts:\n",
      "  [0] Gradient Descent (seed of Gradient Descent)\n",
      "      An optimization algorithm used to minimize a function by iteratively moving towa...\n",
      "  [1] L2 Regularization (prerequisite of Gradient Descent)\n",
      "      A technique that adds a penalty equal to the square of the magnitude of coeffici...\n",
      "  [1] Loss Function (prerequisite of Gradient Descent)\n",
      "      A mathematical function that quantifies the difference between predicted and act...\n",
      "  [1] Gradients (prerequisite of Gradient Descent)\n",
      "      A vector of partial derivatives indicating the direction and rate of the steepes...\n",
      "  [1] Back-Propagation (prerequisite of Gradient Descent)\n",
      "      An algorithm used to compute the gradient of the loss function with respect to t...\n",
      "  [1] Matrix Derivatives (prerequisite of Gradient Descent)\n",
      "      The derivative of a function with respect to a matrix, often represented as a te...\n",
      "  [1] Chain Rule (prerequisite of Gradient Descent)\n",
      "      The chain rule is a formula for computing the derivative of the composition of f...\n",
      "  [1] Non-linear Regression (prerequisite of Gradient Descent)\n",
      "      A statistical method used to model the relationship between a dependent variable...\n",
      "  [1] Decorrelation (prerequisite of Gradient Descent)\n",
      "      The process of transforming data so that the covariance between dimensions becom...\n",
      "  [1] Cost function (prerequisite of Gradient Descent)\n",
      "      A function that measures the performance of a machine learning model by quantify...\n",
      "  [1] Mean Subtraction (prerequisite of Gradient Descent)\n",
      "      A preprocessing technique where the mean of each feature is subtracted from the ...\n",
      "  [1] Self-supervised Learning (prerequisite of Gradient Descent)\n",
      "      A type of machine learning where the model is trained on a labeled dataset, mean...\n",
      "  [1] Gradient Calculation (prerequisite of Gradient Descent)\n",
      "      The process of computing the derivative of the loss function with respect to the...\n",
      "  [1] Hessian Matrix (prerequisite of Gradient Descent)\n",
      "      A square matrix of second-order partial derivatives of a scalar-valued function,...\n",
      "  [1] Vanishing Gradients (prerequisite of Gradient Descent)\n",
      "      Vanishing Gradient is a problem in training deep neural networks where gradients...\n",
      "\n",
      "Retrieved 61 resources:\n",
      "  [pdf] https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/readings/cs224n-2019-notes06-NMT_seq2seq_attention.pdf\n",
      "      Explains: Gradient Descent, Back-Propagation\n",
      "  [pdf] https://arxiv.org/pdf/1901.06958.pdf\n",
      "      Explains: Gradient Descent, Back-Propagation, Self-supervised Learning\n",
      "  [web] https://pantelis.github.io/aiml-common/lectures/nlp/language-models/simple-rnn-language-model/index.html\n",
      "      Explains: Gradient Descent, Loss Function, Back-Propagation\n",
      "  [pdf] http://proceedings.mlr.press/v28/pascanu13.pdf\n",
      "      Explains: Gradient Descent, L2 Regularization, Gradients\n",
      "  [pdf] https://arxiv.org/pdf/1506.00019.pdf\n",
      "      Explains: Gradient Descent, L2 Regularization, Loss Function\n",
      "\n",
      "Retrieved 30 examples:\n",
      "  [case_study] Gradient Descent\n",
      "      Training a neuron involves minimizing a loss function by adjusting weights and b...\n",
      "  [case_study] Gradient Descent\n",
      "      A blog post providing an overview of various algorithms considered enhancements ...\n",
      "  [case_study] L2 Regularization\n",
      "      The model without dropout regularizer achieved an error rate of 14.51% for the C...\n",
      "  [case_study] L2 Regularization\n",
      "      Using weight decay with Œª=0.001 to regularize a CNN model....\n",
      "  [case_study] Back-Propagation\n",
      "      Using the Jacobian matrix to understand how local gradients are calculated in ne...\n",
      "\n",
      "Prerequisite chains:\n",
      "  Overfitting ‚Üí L2 Regularization ‚Üí Gradient Descent\n",
      "\n",
      "Topological order (simple ‚Üí complex):\n",
      "  1. L2 Regularization\n",
      "  2. Non-linear Regression\n",
      "  3. Decorrelation\n",
      "  4. Cost function\n",
      "  5. Mean Subtraction\n",
      "  6. Gradient Calculation\n",
      "  7. Vanishing Gradients\n",
      "  8. Gradient Descent\n",
      "  9. Loss Function\n",
      "  10. Gradients\n",
      "  11. Back-Propagation\n",
      "  12. Matrix Derivatives\n",
      "  13. Chain Rule\n",
      "  14. Self-supervised Learning\n",
      "  15. Hessian Matrix\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Test Graph Retrieval\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/app')\n",
    "\n",
    "from src.retrieval.graph_retriever import GraphRetriever\n",
    "\n",
    "retriever = GraphRetriever(neo4j_uri=\"bolt://neo4j:7687\")\n",
    "\n",
    "# Test with a concept\n",
    "seed_concepts = [\"Gradient Descent\"]\n",
    "subgraph = retriever.expand_seeds(seed_concepts)\n",
    "\n",
    "print(f\"Seed concepts: {subgraph.seed_concepts}\")\n",
    "print(f\"\\nRetrieved {len(subgraph.concepts)} concepts:\")\n",
    "for c in subgraph.concepts:\n",
    "    print(f\"  [{c.depth}] {c.title} ({c.relation_to_seed} of {c.seed_concept})\")\n",
    "    print(f\"      {c.definition[:80]}...\" if c.definition else \"      (no definition)\")\n",
    "\n",
    "print(f\"\\nRetrieved {len(subgraph.resources)} resources:\")\n",
    "for r in subgraph.resources[:5]:\n",
    "    print(f\"  [{r.resource_type}] {r.title}\")\n",
    "    print(f\"      Explains: {', '.join(r.concepts_explained[:3])}\")\n",
    "\n",
    "print(f\"\\nRetrieved {len(subgraph.examples)} examples:\")\n",
    "for e in subgraph.examples[:5]:\n",
    "    print(f\"  [{e.example_type}] {e.concept}\")\n",
    "    print(f\"      {e.text[:80]}...\")\n",
    "\n",
    "print(f\"\\nPrerequisite chains:\")\n",
    "for chain in subgraph.prereq_chain:\n",
    "    print(f\"  {' ‚Üí '.join(chain)}\")\n",
    "\n",
    "# Get topological order for explanation\n",
    "order = retriever.get_topological_order(subgraph.concepts)\n",
    "print(f\"\\nTopological order (simple ‚Üí complex):\")\n",
    "for i, title in enumerate(order, 1):\n",
    "    print(f\"  {i}. {title}\")\n",
    "\n",
    "retriever.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2c685e",
   "metadata": {},
   "source": [
    "## Hybrid Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7ca2340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2...\n",
      "Model loaded. Embedding dimension: 384\n",
      "============================================================\n",
      "Query: How does backpropagation work in neural networks?\n",
      "============================================================\n",
      "\n",
      "[1] Retrieving context...\n",
      "Query: How does backpropagation work in neural networks?\n",
      "Seeds: Back-Propagation, Backward Propagation, Stochastic Backpropagation, Backward Pass, Forward Propagation\n",
      "Concepts: 15\n",
      "Resources: 58\n",
      "Examples: 27\n",
      "Order: Backward Propagation ‚Üí Stochastic Backpropagation ‚Üí Forward Propagation ‚Üí Residual Networks ‚Üí Recurrent Neural Networks (RNNs)...\n",
      "\n",
      "[2] Seed concepts found:\n",
      "   - Back-Propagation (score: 0.6162)\n",
      "   - Backward Propagation (score: 0.5871)\n",
      "   - Stochastic Backpropagation (score: 0.5814)\n",
      "   - Backward Pass (score: 0.5661)\n",
      "   - Forward Propagation (score: 0.5596)\n",
      "\n",
      "[3] Explanation order:\n",
      "   1. Backward Propagation\n",
      "   2. Stochastic Backpropagation\n",
      "   3. Forward Propagation\n",
      "   4. Residual Networks\n",
      "   5. Recurrent Neural Networks (RNNs)\n",
      "   6. Hard Attention\n",
      "   7. Feedforward Networks\n",
      "   8. Back-Propagation\n",
      "\n",
      "[4] Generating answer...\n",
      "\n",
      "============================================================\n",
      "ERICA'S ANSWER:\n",
      "============================================================\n",
      "Certainly! Let's start with the foundational concepts and build up to understanding backpropagation in neural networks.\n",
      "\n",
      "### Foundational Concepts\n",
      "\n",
      "1. **Feedforward Networks**\n",
      "   - A feedforward neural network is a type of neural network where information moves in only one direction, from input through hidden layers to output, with no cycles or loops. This is the simplest type of neural network.\n",
      "   - **Example**: A feedforward neural network is illustrated in Figure 2, where an example is presented to the network by setting the values of the input nodes. [Resource: http://cs231n.stanford.edu/handouts/derivatives.pdf]\n",
      "\n",
      "2. **Forward Pass**\n",
      "   - The forward pass is the process of computing the output of a neural network given an input, propagating data through the network layer by layer.\n",
      "   - **Example**: A forward pass is implemented to compute the output of the neural network. [Resource: https://pantelis.github.io/aiml-common/lectures/nlp/nlp-introduction/word2vec/word2vec_from_scratch.html]\n",
      "\n",
      "3. **Loss Function**\n",
      "   - A loss function is a mathematical function that quantifies the difference between predicted and actual values, used to guide the learning process in machine learning.\n",
      "   - **Example**: The model uses the negative log-likelihood loss function (NLLLoss) for training. [Resource: https://pantelis.github.io/aiml-common/lectures/optimization/regularization/index.html]\n",
      "\n",
      "4. **Gradients**\n",
      "   - A gradient is a vector of partial derivatives indicating the direction and rate of the steepest increase in a function, used in optimization algorithms like gradient descent.\n",
      "   - **Example**: Computing the gradient of the 'African elephant' class with respect to the output feature map of `block5_conv3`. [Resource: http://cs231n.stanford.edu/handouts/derivatives.pdf]\n",
      "\n",
      "5. **Chain Rule**\n",
      "   - The chain rule is a formula for computing the derivative of the composition of functions. In the scalar case, it states that the derivative of \\( z \\) with respect to \\( x \\) is the product of the derivative of \\( z \\) with respect to \\( y \\) and the derivative of \\( y \\) with respect to \\( x \\).\n",
      "   - **Example**: Deriving the Jacobian matrix for the function \\( g(x) = [g_1(f_1(x), f_2(x)), g_2(f_1(x), f_2(x))] \\) using the chain rule. [Resource: http://cs231n.stanford.edu/handouts/derivatives.pdf]\n",
      "\n",
      "### Intermediate Concepts\n",
      "\n",
      "6. **Backward Pass**\n",
      "   - The backward pass is the process of computing gradients of the loss function with respect to the parameters of a neural network, performed during backpropagation.\n",
      "   - **Example**: Training a neural network language model on the entire text of Wikipedia using backpropagation through time would be computationally expensive and slow. [Resource: https://pantelis.github.io/aiml-common/lectures/rnn/lstm/index.html]\n",
      "\n",
      "7. **Back-Propagation**\n",
      "   - Backpropagation is an algorithm used to compute the gradient of the loss function with respect to the weights in a neural network, enabling the optimization of the network during training.\n",
      "   - **Example**: Using the Jacobian matrix to understand how local gradients are calculated in neural network backpropagation. [Resource: http://cs231n.stanford.edu/handouts/derivatives.pdf]\n",
      "\n",
      "### Advanced Concepts\n",
      "\n",
      "8. **Stochastic Backpropagation**\n",
      "   - Stochastic backpropagation is an optimization method used in neural network training that is faster for large and redundant problems, such as classification tasks.\n",
      "   - **Example**: Gradients are computed using backpropagation and then clipped to prevent exploding gradients. [Resource: https://arxiv.org/pdf/1412.6806.pdf]\n",
      "\n",
      "9. **Residual Networks**\n",
      "   - Residual networks (ResNets) are a type of neural network that uses skip connections to allow the training of deeper networks by enabling gradients to flow through the network more effectively.\n",
      "   - **Example**: Removing single layers from residual networks at test time does not noticeably affect performance, unlike traditional architectures like VGG. [Resource: https://arxiv.org/pdf/1605.06431.pdf]\n",
      "\n",
      "10. **Recurrent Neural Networks (RNNs)**\n",
      "    - Recurrent neural networks (RNNs) are a type of neural network designed to handle sequential data by maintaining an internal hidden state that is updated as the network processes each input in the sequence.\n",
      "    - **Example**: An RNN model using word embeddings and a lattice representation of the decoder output achieved a BLEU score of 28.5 on French-English translation tasks. [Resource: https://pantelis.github.io/aiml-common/lectures/rnn/lstm/index.html]\n",
      "\n",
      "### Understanding Backpropagation\n",
      "\n",
      "Now that we have covered the foundational and intermediate concepts, let's dive into backpropagation.\n",
      "\n",
      "#### What is Backpropagation?\n",
      "\n",
      "Backpropagation is a crucial algorithm in training neural networks. It is used to efficiently compute the gradients of the loss function with respect to the weights of the network. These gradients are then used to update the weights using an optimization algorithm like gradient descent.\n",
      "\n",
      "#### How Does Backpropagation Work?\n",
      "\n",
      "1. **Forward Pass**:\n",
      "   - The forward pass involves computing the output of the neural network given an input. This is done by propagating the input through each layer of the network, applying the weights and activation functions.\n",
      "   - **Example**: A forward pass is implemented to compute the output of the neural network. [Resource: https://pantelis.github.io/aiml-common/lectures/nlp/nlp-introduction/word2vec/word2vec_from_scratch.html]\n",
      "\n",
      "2. **Compute Loss**:\n",
      "   - After the forward pass, the loss function is used to compute the difference between the predicted output and the actual output (ground truth).\n",
      "   - **Example**: The model uses cross entropy loss to measure the difference between the predicted output sequence and the ground truth sequence. [Resource: https://pantelis.github.io/aiml-common/lectures/nlp/nlp-introduction/word2vec/word2vec_from_scratch.html]\n",
      "\n",
      "3. **Backward Pass**:\n",
      "   - The backward pass involves computing the gradients of the loss function with respect to the weights of the network. This is done using the chain rule of calculus.\n",
      "   - **Example**: The backward pass in LSTM allows gradients to propagate through many time steps without exploding or vanishing due to the constant error carousel. [Resource: https://pantelis.github.io/aiml-common/lectures/rnn/lstm/index.html]\n",
      "\n",
      "4. **Update Weights**:\n",
      "   - The gradients are used to update the weights of the network using an optimization algorithm like gradient descent. The weights are adjusted in the direction that minimizes the loss function.\n",
      "   - **Example**: Training a neuron involves minimizing a loss function by adjusting weights and bias using gradient descent. [Resource: https://pantelis.github.io/aiml-common/lectures/optimization/regularization/index.html]\n",
      "\n",
      "#### Step-by-Step Example\n",
      "\n",
      "Let's walk through a simple example using a feedforward neural network with one hidden layer.\n",
      "\n",
      "1. **Network Architecture**:\n",
      "   - Input layer with 2 neurons\n",
      "   - Hidden layer with 3 neurons\n",
      "   - Output layer with 1 neuron\n",
      "\n",
      "2. **Forward Pass**:\n",
      "   - Input: \\( x = [x_1, x_2] \\)\n",
      "   - Weights: \\( W_1 \\) (input to hidden), \\( W_2 \\) (hidden to output)\n",
      "   - Bias: \\( b_1 \\) (input to hidden), \\( b_2 \\) (hidden to output)\n",
      "   - Activation function: Sigmoid\n",
      "\n",
      "   \\[\n",
      "   z_1 = x \\cdot W_1 + b_1\n",
      "   \\]\n",
      "   \\[\n",
      "   a_1 = \\sigma(z_1)\n",
      "   \\]\n",
      "   \\[\n",
      "   z_2 = a_1 \\cdot W_2 + b_2\n",
      "   \\]\n",
      "   \\[\n",
      "   \\hat{y} = \\sigma(z_2)\n",
      "   \\]\n",
      "\n",
      "3. **Compute Loss**:\n",
      "   - Loss function: Mean Squared Error (MSE)\n",
      "   - True output: \\( y \\)\n",
      "\n",
      "   \\[\n",
      "   L = \\frac{1}{2} (y - \\hat{y})^2\n",
      "   \\]\n",
      "\n",
      "4. **Backward Pass**:\n",
      "   - Compute the gradient of the loss with respect to the output:\n",
      "     \\[\n",
      "     \\frac{\\partial L}{\\partial \\hat{y}} = \\hat{y} - y\n",
      "     \\]\n",
      "   - Compute the gradient of the output with respect to the weighted sum:\n",
      "     \\[\n",
      "     \\frac{\\partial \\hat{y}}{\\partial z_2} = \\sigma'(z_2)\n",
      "     \\]\n",
      "   - Compute the gradient of the loss with respect to the weighted sum:\n",
      "     \\[\n",
      "     \\frac{\\partial L}{\\partial z_2} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_2}\n",
      "     \\]\n",
      "   - Compute the gradient of the weighted sum with respect to the weights and biases:\n",
      "     \\[\n",
      "     \\frac{\\partial z_2}{\\partial W_2} = a_1\n",
      "     \\]\n",
      "     \\[\n",
      "     \\frac{\\partial z_2}{\\partial b_2} = 1\n",
      "     \\]\n",
      "   - Compute the gradients of the loss with respect to the weights and biases:\n",
      "     \\[\n",
      "     \\frac{\\\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# M4: Full Retrieval + Generation Pipeline Test\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/app')\n",
    "\n",
    "from src.retrieval.hybrid_retriever import HybridRetriever\n",
    "from src.generation.answer_generator import AnswerGenerator\n",
    "\n",
    "# Initialize components\n",
    "retriever = HybridRetriever(\n",
    "    mongo_uri=\"mongodb://erica:erica_password_123@mongodb:27017/\",\n",
    "    chroma_host=\"chromadb\",\n",
    "    neo4j_uri=\"bolt://neo4j:7687\",\n",
    ")\n",
    "\n",
    "generator = AnswerGenerator()  # Uses OPENROUTER_API_KEY from environment\n",
    "\n",
    "# Test query\n",
    "query = \"How does backpropagation work in neural networks?\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Query: {query}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Retrieve\n",
    "print(\"\\n[1] Retrieving context...\")\n",
    "result = retriever.retrieve(query)\n",
    "print(result.summary())\n",
    "\n",
    "# Step 2: Show what we're sending to the LLM\n",
    "print(\"\\n[2] Seed concepts found:\")\n",
    "for match in result.semantic_matches:\n",
    "    print(f\"   - {match['title']} (score: {match['score']})\")\n",
    "\n",
    "print(\"\\n[3] Explanation order:\")\n",
    "for i, title in enumerate(result.ordered_concepts[:8], 1):\n",
    "    print(f\"   {i}. {title}\")\n",
    "\n",
    "# Step 3: Generate answer\n",
    "print(\"\\n[4] Generating answer...\")\n",
    "answer = generator.generate(result)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ERICA'S ANSWER:\")\n",
    "print(\"=\" * 60)\n",
    "print(answer)\n",
    "\n",
    "# Cleanup\n",
    "retriever.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e86cf5",
   "metadata": {},
   "source": [
    "## Launch Erica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffc8e844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streamlit app starting at http://localhost:8501\n",
      "Press Ctrl+C to stop\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Launch Streamlit App\n",
    "# =============================================================================\n",
    "\n",
    "import subprocess\n",
    "import webbrowser\n",
    "\n",
    "# Start Streamlit in background\n",
    "process = subprocess.Popen([\n",
    "    \"streamlit\", \"run\", \"src/app.py\",\n",
    "    \"--server.port\", \"8501\",\n",
    "    \"--server.address\", \"0.0.0.0\"\n",
    "])\n",
    "\n",
    "print(\"Streamlit app starting at http://localhost:8501\")\n",
    "print(\"Press Ctrl+C to stop\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
